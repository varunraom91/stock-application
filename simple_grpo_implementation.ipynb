{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQoDbNvKQFxq4JDGREnWAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunraom91/stock-application/blob/main/simple_grpo_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXsg2JXMGQhv",
        "outputId": "abf616b9-e713-4414-b045-73f2b952cf76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== PPO Training =====\n",
            "PPO Advantage: 1.40, Loss: -1.41\n",
            "PPO Advantage: 1.40, Loss: -1.41\n",
            "PPO Advantage: 1.40, Loss: -1.41\n",
            "\n",
            "===== GRPO Training =====\n",
            "GRPO Group Rewards: [0.0, 1.0, 1.0]\n",
            "GRPO Normalized: [-1.41, 0.71, 0.71]\n",
            "GRPO Loss: -0.00\n",
            "\n",
            "GRPO Group Rewards: [1.0, 0.0, 1.0]\n",
            "GRPO Normalized: [0.71, -1.41, 0.71]\n",
            "GRPO Loss: -0.00\n",
            "\n",
            "GRPO Group Rewards: [0.0, 1.0, 1.0]\n",
            "GRPO Normalized: [-1.41, 0.71, 0.71]\n",
            "GRPO Loss: -0.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# =============================================\n",
        "# Environment: Math Equation Solver\n",
        "# =============================================\n",
        "class MathEnv:\n",
        "    def __init__(self):\n",
        "        self.equation = \"2x + 3 = 7\"\n",
        "        self.solution = {\"steps\": [\"2x = 4\", \"x = 2\"], \"reward\": 1.0}  # Ground truth\n",
        "\n",
        "    def get_reward(self, generated_steps):\n",
        "        # Reward = 1 if steps match solution, 0.5 for partial correctness, 0 otherwise\n",
        "        if generated_steps == self.solution[\"steps\"]:\n",
        "            return 1.0\n",
        "        elif any(step in generated_steps for step in self.solution[\"steps\"]):\n",
        "            return 0.5\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "# =============================================\n",
        "# Neural Networks\n",
        "# =============================================\n",
        "class PolicyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 64),  # Simplified \"understanding\" of equation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 4)     # 4 possible actions: [\"add\", \"subtract\", \"multiply\", \"divide\"]\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "class ValueModel(nn.Module):  # Used only by PPO\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "# =============================================\n",
        "# PPO Training (with Value Model)\n",
        "# =============================================\n",
        "def run_ppo(env, num_iterations=3):\n",
        "    policy = PolicyModel()\n",
        "    value_model = ValueModel()\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)\n",
        "\n",
        "    # Dummy \"state\" representing equation (random for illustration)\n",
        "    state = torch.randn(128)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Generate solution steps\n",
        "        action_logits = policy(state)\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "\n",
        "        # Simulate generating steps (e.g., [\"subtract 3\", \"divide by 2\"])\n",
        "        generated_steps = [\"2x = 4\", \"x = 2\"]  # Assume perfect generation for demo\n",
        "\n",
        "        # Get reward (1.0 in this case)\n",
        "        reward = env.get_reward(generated_steps)\n",
        "\n",
        "        # PPO Advantage Calculation\n",
        "        value_pred = value_model(state)  # Value model estimates reward\n",
        "        advantage = reward - value_pred.detach()\n",
        "\n",
        "        # PPO Loss (simplified)\n",
        "        old_probs = torch.tensor([0.2, 0.3, 0.1, 0.4])  # Placeholder\n",
        "        new_probs = action_probs\n",
        "        ratio = new_probs / old_probs\n",
        "        clipped_ratio = torch.clamp(ratio, 0.8, 1.2)\n",
        "        ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
        "\n",
        "        print(f\"PPO Advantage: {advantage.item():.2f}, Loss: {ppo_loss.item():.2f}\")\n",
        "\n",
        "# =============================================\n",
        "# GRPO Training (Group-Based)\n",
        "# =============================================\n",
        "def run_grpo(env, group_size=3, num_iterations=3):\n",
        "    policy = PolicyModel()\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)\n",
        "\n",
        "    state = torch.randn(128)  # Same \"equation\" representation\n",
        "\n",
        "    # Initialize old_probs before the loop\n",
        "    old_probs = torch.softmax(policy(state), dim=-1)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Generate group of solutions\n",
        "        group_rewards = []\n",
        "        for _ in range(group_size):\n",
        "            action_logits = policy(state)\n",
        "            action_probs = torch.softmax(action_logits, dim=-1)\n",
        "\n",
        "            # Simulate different solutions (2 correct, 1 incorrect)\n",
        "            generated_steps = [\n",
        "                [\"2x = 4\", \"x = 2\"],  # Correct\n",
        "                [\"2x = 4\", \"x = 2\"],  # Correct\n",
        "                [\"x = 7\"]             # Incorrect\n",
        "            ][np.random.choice([0,1,2])]\n",
        "\n",
        "            reward = env.get_reward(generated_steps)\n",
        "            group_rewards.append(reward)\n",
        "\n",
        "        # GRPO Advantage Calculation\n",
        "        group_mean = np.mean(group_rewards)\n",
        "        group_std = np.std(group_rewards) + 1e-8\n",
        "        normalized_rewards = [(r - group_mean)/group_std for r in group_rewards]\n",
        "\n",
        "        # GRPO Loss (with KL penalty)\n",
        "        new_probs = torch.softmax(policy(state), dim=-1)\n",
        "        kl_penalty = 0.1 * torch.sum(new_probs * torch.log(new_probs/old_probs))\n",
        "        grpo_loss = -torch.mean(torch.tensor(normalized_rewards)) + kl_penalty\n",
        "\n",
        "        print(f\"GRPO Group Rewards: {group_rewards}\")\n",
        "        print(f\"GRPO Normalized: {[round(r,2) for r in normalized_rewards]}\")\n",
        "        print(f\"GRPO Loss: {grpo_loss.item():.2f}\\n\")\n",
        "\n",
        "        # Update old_probs for the next iteration\n",
        "        old_probs = new_probs.detach()\n",
        "\n",
        "# =============================================\n",
        "# Run Both Algorithms\n",
        "# =============================================\n",
        "if __name__ == \"__main__\":\n",
        "    env = MathEnv()\n",
        "    print(\"===== PPO Training =====\")\n",
        "    run_ppo(env)\n",
        "\n",
        "    print(\"\\n===== GRPO Training =====\")\n",
        "    run_grpo(env)"
      ]
    }
  ]
}